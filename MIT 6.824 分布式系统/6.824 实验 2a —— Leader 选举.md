# 前言

6.824 的实验二，是实现 Raft 算法，在后续实验中的实现的分布式 KV 存储会将本实验实现的 Raft 算法作为分布式共识模块使用，所以实验二对后续实验至关重要。

实验二将整个 Raft 算法分为四个步骤，作为四个子实验去实现。实验 2a 只实现基本的 Leader 选举和心跳，来保证在各种极端（断线）场景下都可以正常地换届和选举。

Of course，2a 作为奠定整个四个子实验基础的起始实验，不仅仅需要实现 Leader 选举功能，更需要搭好整体的流程处理的框架。同样，我实现的是无锁版本，Raft 结构体里的 mu 变量可以删掉啦（癫狂

# 实验讲解

实验指导书在 [https://pdos.csail.mit.edu/6.824/labs/lab-raft.html](https://pdos.csail.mit.edu/6.824/labs/lab-raft.html)。和实验一不一样，这次完全没有任何参考。我们需要实现的代码在 `src/raft/raft.go` 中，这个 Raft 结构体只有一个很基础的结构体：

```go
type Raft struct {
	peers []*labrpc.ClientEnd // RPC end points of all peers
	persister *Persister // Object to hold this peer's persisted state
	me int // this peer's index into peers[]
	dead int32 // set by Kill()
}
```

每一个 Raft 结构体都是集群中的一个 Server，Raft 结构体需要存储该 Server 所有需要的内容。

其中 peers 是当前配置集群的所有 server，ClientEnd 结构体可以通过调用 Call 直接发送 RPC 请求，me 则是当前机器在集群中的唯一标识，再其他机器上也是认这个 index 的。

lab2a 中 Raft 的入口是 `Make()` 方法，在 Make 方法初始化完成结构体后，会启动一个协程 `rf.ticker()`，该协程会执行一个无限循环（其实是根据结束标识持续执行的循环，鉴于我们不关心机器被关闭后的事情，所以可以看作无限循环）

实验二最难的地方就在于，框架实现的内容太少了，我们基本需要从零实现整个 Raft 算法。好在，论文中的 Figure 2 基本已经给出了整体的实现思路。

另外，测试用例的实现在同文件夹下的 `test_test.go` 中，如果测试用例不通过，可以看一下测试用例的实现，根据测试场景来 debug。

# 实验思路

亥铣